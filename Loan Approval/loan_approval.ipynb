import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

data = pd.read_csv('data/Loan_Approval.csv')
data.head()

# Data Wrangling
# Dealing with null value

data.isna().sum(), data.shape
data.dropna(axis=0, inplace=True)
data.isna().sum(), data.shape

# Converting categorical data

data.drop('Loan_ID', axis=1, inplace=True)
data.info()
property_area_lookup = dict(zip(data['Property_Area'].unique(), [0, 1, 2]))
dependecy_lookup = dict(zip(data['Dependents'].unique(), [0, 1, 2, 3, 4]))

data['Loan_Status'] = pd.get_dummies(data['Loan_Status'], drop_first=True)
data['Property_Area'].replace(property_area_lookup, inplace=True)
data['Gender'] = pd.get_dummies(data['Gender'], drop_first=True)
data['Dependents'].replace(dependecy_lookup, inplace=True)
data['Married'] = pd.get_dummies(data['Married'], drop_first=True)
data['Education'] = pd.get_dummies(data['Education'], drop_first=True)
data['Self_Employed'] = pd.get_dummies(data['Self_Employed'], drop_first=True)

data.info()

print(property_area_lookup)
print(dependecy_lookup)

data.head()

# Explanatory Data Analysis
# Correlation Analysis With Heatmap

plt.figure(figsize=(16, 10))

# applying mask
mask = np.triu(np.ones_like(data.corr()))
 
# plotting a triangle correlation heatmap rocket_r, viridis
sns.heatmap(data.corr(), mask=mask, annot=True, cmap='rocket_r', linewidth=0.1)
# Gender Loan Distribution

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

sns.barplot(x='Gender', y='Loan_Status', data=data, estimator=np.std, ax=ax[0])
ax[0].legend(['Male'])

sns.barplot(x='Married', y='Loan_Status', data=data, estimator=np.std, ax=ax[1])
ax[1].legend(['Married'])
# 
plt.figure(figsize=(15, 8))
sns.pairplot(data, hue='Gender')
sns.violinplot(x='Gender', y='Loan_Status', data=data, hue='Married')
# Distplot Loan Amount

sns.displot(data['LoanAmount'], kde=False, bins=35)
sns.scatterplot(x=data['ApplicantIncome'], y=data['LoanAmount'], hue=data['Gender'])
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))

sns.scatterplot(x=data['LoanAmount'], y=data['ApplicantIncome'], hue=data['Gender'], ax=ax[0])

sns.scatterplot(x=data['CoapplicantIncome'], y=data['ApplicantIncome'], hue=data['Gender'], ax=ax[1])
sns.lmplot(x='LoanAmount', y='ApplicantIncome',  data=data,  hue='Gender')

sns.lmplot(x='CoapplicantIncome', y='ApplicantIncome', data=data, hue='Gender')

sns.jointplot(x='LoanAmount', y='ApplicantIncome', data=data, kind='kde')

# Design Model

from sklearn.model_selection import train_test_split
X = data.drop('Loan_Status', axis=1)
y = data['Loan_Status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, y_train.shape

from sklearn.linear_model import LogisticRegressionDClassifier, LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

def nneighbors():
    
    knn = KNeighborsClassifier(n_neighbors=13).fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    train_score = knn.score(X_train, y_train)
    test_score = knn.score(X_test, y_test)
    
    print(f'Training Score: {train_score}, Test Score: {test_score}')
    
    return {
        'model': knn,
        'y_pred': y_pred,
        'train_score': train_score,
        'test_score': test_score
    }
    
nneighbors()

def naive():
    
    clf = GaussianNB().fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    
    print(f'Training Score: {train_score}, Test Score: {test_score}')
    
    return {
        'model': clf,
        'y_pred': y_pred,
        'train_score': train_score,
        'test_score': test_score
    }
    
naive()

def svm():
    
    svm = SVC(kernel='linear').fit(X_train, y_train)
    y_pred = svm.predict(X_test)
    train_score = svm.score(X_train, y_train)
    test_score = svm.score(X_test, y_test)
    
    print(f'Training Score: {train_score}, Test Score: {test_score}')
    
    return {
        'model': svm,
        'y_pred': y_pred,
        'train_score': train_score,
        'test_score': test_score
    }
    
svm()

def logistic():
    
    clf = LogisticRegression().fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    
    print(f'Training Score: {train_score}, Test Score: {test_score}')
    
    return {
        'model': clf,
        'y_pred': y_pred,
        'train_score': train_score,
        'test_score': test_score
    }
    
logistic()

def sgd():
    
    clf =  SGDClassifier(max_iter=1000, tol=1e-3).fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    train_score = clf.score(X_train, y_train)
    test_score = clf.score(X_test, y_test)
    
    print(f'Training Score: {train_score}, Test Score: {test_score}')
    
    return {
        'model': clf,
        'y_pred': y_pred,
        'train_score': train_score,
        'test_score': test_score
    }
    
sgd()

data.head()

from sklearn.metrics import classification_report, confusion_matrix

clf = logistic()
y_pred = clf['y_pred']
print(classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)
